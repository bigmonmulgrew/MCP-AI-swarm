# Copied over from another project and contains some redundant settings still to be cleaned up.

### Configuration environemnt for docker stack
# _I represents internal
# _E represents external
#
# It is recommended to change the ports from the default for security reasons.

# Location of LLMS, Ollama or OpenAI.
LLM_LOCATION_ENV=ollama
EMBED_LOCATION_ENV=ollama

# Name of local LLM models
LLM_DEFAULT_MODEL=phi4
LLM_MAX_CTX_MODEL=phi4
LLM_EMBEDDING_DEFAULT_MODEL="tazarov/all-minilm-l6-v2-f32"
LLM_DEFAULT_TEMP=0

# OpenAI
OPENAI_API_KEY=
OPENAI_MODEL=
VITE_SHOW_CHAT_INPUT=false
BASE_HOST=

# Chunk variables
CHUNK_SIZE_ENV=2500
CHUNK_OVERLAP_ENV=0
## Accepted document types [array of text values, e.g. "docx","pdf"]
DOC_TYPES=

# Add searchable images to PDFs and save the output
OPTIMISE_IMAGE_PDFS=True
PDF_OUTPUT_FOLDER="./optimised"

# Vector search chunk number return
VECTOR_TOP_N=6

EXTERNAL_LICENCE_SERVER=

# Frontend server if not running as a standalone demo
FRONTEND_SERVER=

# Integration server, if not running as a standalone demo
BACKEND_SERVER=