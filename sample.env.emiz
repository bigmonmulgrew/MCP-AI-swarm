# Copied over from another project and contains some redundant settings still to be cleaned up.

### Configuration environemnt for docker stack
# _I represents internal
# _E represents external
#
# It is recommended to change the ports from the default for security reasons.

## Api container configuration
# API port number to use inside the container
API_PORT_I=3000
# API port number to expose outside the container
API_PORT_E=8085

## Ollama container configuration
OLLAMA_PORT_I=11434
OLLAMA_PORT_E=11434

## Frontend config
#Frontend ports
FRONTEND_I=5173
FRONTEND_E=5173

# Location of LLMS, Ollama or OpenAI.
LLM_LOCATION_ENV=
EMBED_LOCATION_ENV=

# Name of local LLM models
LLM_DEFAULT_MODEL=
LLM_MAX_CTX_MODEL=
LLM_EMBEDDING_DEFAULT_MODEL=
LLM_DEFAULT_TEMP=

# OpenAI
OPENAI_API_KEY=
OPENAI_MODEL=

VITE_SHOW_CHAT_INPUT=false

BASE_HOST=

# Chunk variables
CHUNK_SIZE_ENV=
CHUNK_OVERLAP_ENV=
## Accepted document types [array of text values, e.g. "docx","pdf"]
DOC_TYPES=

# Add searchable images to PDFs and save the output
OPTIMISE_IMAGE_PDFS=True
PDF_OUTPUT_FOLDER="./optimised"

# Vector search chunk number return
VECTOR_TOP_N=

EXTERNAL_LICENCE_SERVER=

# Frontend server if not running as a standalone demo
FRONTEND_SERVER=

# Integration server, if not running as a standalone demo
BACKEND_SERVER=